# -*- coding: utf-8 -*-
"""Quora_question_similarity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DFZ8YsjDpBSDiAti0fPB44FGF0FSpxDB
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import nltk
import string
import re
import seaborn as sns

data = pd.read_csv(r'D:\ML_Projects\Quora-Question-Similarity-Pair\data\train.csv')

"""## Basic Data Exploration"""

print(data.shape)

data.head()

data.isnull().sum()

data = data.dropna()

data.shape

data.isnull().sum()

data.describe()

data.info()

print(data.is_duplicate.value_counts())
print(data.is_duplicate.value_counts()/data.is_duplicate.value_counts().sum()*100)

"""duplicate question containing data point are 36.92% and non duplicate containing data point are 63.08%"""

qids = np.append(data.qid1.values , data.qid2.values)
qids.shape

print("total no of unique question in data set: ", len(set(qids)))

occurences = np.bincount(qids)
plt.figure(figsize=(10,5)) 
plt.hist(occurences, bins=range(0,160))
plt.yscale('log')
plt.xlabel('Number of times question repeated')
plt.ylabel('Number of questions')
plt.title('Question vs Repeatition')
plt.show()
print("Minimun occurences of any question: " , np.min(occurences))
print("Maximum occurences of any question: " , np.max(occurences))

"""## Data preprocessing"""

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

class TextPreprocessing:
    
    def __init__(self , data):
        self.data = data
        self.my_stopword = list(stopwords.words('english'))
        self.my_lemmatizer = WordNetLemmatizer()
    
    def text_preprocessing(self , data , column_name):
        
        data_copy = data.copy()
        
        data_copy[column_name] = data_copy[column_name].apply(self.lower_text)
        data_copy[column_name] = data_copy[column_name].apply(self.remove_punctuation)
        data_copy[column_name] = data_copy[column_name].apply(self.replace_numeric_to_string)
        data_copy[column_name] = data_copy[column_name].apply(self.remove_urls)
        data_copy[column_name] = data_copy[column_name].apply(self.replace_special_character_to_string_equalent)
        data_copy[column_name] = data_copy[column_name].apply(self.decontrate_words)
        data_copy[column_name] = data_copy[column_name].apply(self.remove_stopwords)
        data_copy[column_name] = data_copy[column_name].apply(self.text_lemmatization)
        
        return data_copy
        
    
    def lower_text(self , text):
        return text.lower().strip()
    
    def remove_punctuation(self , text):
        text = text.translate(str.maketrans("", "", string.punctuation)) 
        return text
    
    def replace_numeric_to_string(self, text):
        text = text.replace(',000,000,000 ', 'b ')
        text = text.replace(',000,000 ', 'm ')
        text = text.replace(',000', 'k ')
        text = re.sub(r'([0-9]+)000000000' , r'\1b', text)
        text = re.sub(r'([0-9]+)000000' , r'\1m', text)
        text = re.sub(r'([0-9]+)000' , r'\1k', text)
        return text
    
    def remove_urls(self, text):
        text = re.sub(r"http\S+", "", text)
        return text
    
    def replace_special_character_to_string_equalent(self , text):
        text = text.replace('%' , ' percent')
        text = text.replace('$' , ' dollar')
        text = text.replace('₹' , ' repee')
        text = text.replace('€' , ' euro')
        text = text.replace('@' , ' at')
        return text
    
    def remove_stopwords(self , text):
        words = text.split()
        new_words = [word for word in words if not word in self.my_stopword]
        text = ' '.join(new_words)
        return text
    
    def text_lemmatization(self , text):
        text = ' '.join([self.my_lemmatizer.lemmatize(word) for word in text.split()])
        return text
    
    def decontrate_words(self , text):
        contractions = { 
            "ain't": "am not",
            "aren't": "are not",
            "can't": "can not",
            "can't've": "can not have",
            "'cause": "because",
            "could've": "could have",
            "couldn't": "could not",
            "couldn't've": "could not have",
            "didn't": "did not",
            "doesn't": "does not",
            "don't": "do not",
            "hadn't": "had not",
            "hadn't've": "had not have",
            "hasn't": "has not",
            "haven't": "have not",
            "he'd": "he would",
            "he'd've": "he would have",
            "he'll": "he will",
            "he'll've": "he will have",
            "he's": "he is",
            "how'd": "how did",
            "how'd'y": "how do you",
            "how'll": "how will",
            "how's": "how is",
            "i'd": "i would",
            "i'd've": "i would have",
            "i'll": "i will",
            "i'll've": "i will have",
            "i'm": "i am",
            "i've": "i have",
            "isn't": "is not",
            "it'd": "it would",
            "it'd've": "it would have",
            "it'll": "it will",
            "it'll've": "it will have",
            "it's": "it is",
            "let's": "let us",
            "ma'am": "madam",
            "mayn't": "may not",
            "might've": "might have",
            "mightn't": "might not",
            "mightn't've": "might not have",
            "must've": "must have",
            "mustn't": "must not",
            "mustn't've": "must not have",
            "needn't": "need not",
            "needn't've": "need not have",
            "o'clock": "of the clock",
            "oughtn't": "ought not",
            "oughtn't've": "ought not have",
            "shan't": "shall not",
            "sha'n't": "shall not",
            "shan't've": "shall not have",
            "she'd": "she would",
            "she'd've": "she would have",
            "she'll": "she will",
            "she'll've": "she will have",
            "she's": "she is",
            "should've": "should have",
            "shouldn't": "should not",
            "shouldn't've": "should not have",
            "so've": "so have",
            "so's": "so as",
            "that'd": "that would",
            "that'd've": "that would have",
            "that's": "that is",
            "there'd": "there would",
            "there'd've": "there would have",
            "there's": "there is",
            "they'd": "they would",
            "they'd've": "they would have",
            "they'll": "they will",
            "they'll've": "they will have",
            "they're": "they are",
            "they've": "they have",
            "to've": "to have",
            "wasn't": "was not",
            "we'd": "we would",
            "we'd've": "we would have",
            "we'll": "we will",
            "we'll've": "we will have",
            "we're": "we are",
            "we've": "we have",
            "weren't": "were not",
            "what'll": "what will",
            "what'll've": "what will have",
            "what're": "what are",
            "what's": "what is",
            "what've": "what have",
            "when's": "when is",
            "when've": "when have",
            "where'd": "where did",
            "where's": "where is",
            "where've": "where have",
            "who'll": "who will",
            "who'll've": "who will have",
            "who's": "who is",
            "who've": "who have",
            "why's": "why is",
            "why've": "why have",
            "will've": "will have",
            "won't": "will not",
            "won't've": "will not have",
            "would've": "would have",
            "wouldn't": "would not",
            "wouldn't've": "would not have",
            "y'all": "you all",
            "y'all'd": "you all would",
            "y'all'd've": "you all would have",
            "y'all're": "you all are",
            "y'all've": "you all have",
            "you'd": "you would",
            "you'd've": "you would have",
            "you'll": "you will",
            "you'll've": "you will have",
            "you're": "you are",
            "you've": "you have"
           }
        
        text_decontrate = []
        
        for word in text.split():
            if word in contractions:
                word = contractions[word]
            text_decontrate.append(word)
    
        text = ' '.join(text_decontrate)
        
        return text

tp = TextPreprocessing(data)
data_ = tp.text_preprocessing(data , 'question1')
data__ = tp.text_preprocessing(data_ , 'question2')

data__.head()

"""## Features Engineering"""

from fuzzywuzzy import fuzz

class FeatureEngineering:
    
    def __init__(self , data):
        self.data = data
        
    
    def feature_extraction(self , data):
        
        data_copy = data.copy()
        
        data_copy['length_of_q1'] = data_copy['question1'].apply(lambda text : len(text))
        
        data_copy['length_of_q2'] = data_copy['question2'].apply(lambda text : len(text))
        
        data_copy['total_number_of_words_in_q1'] = data_copy['question1'].apply(lambda text : len(nltk.word_tokenize(text)))
        
        data_copy['total_number_of_words_in_q2'] = data_copy['question2'].apply(lambda text : len(nltk.word_tokenize(text)))
        
        data_copy['sum_of_total_words_of_q1_and_q2'] = data_copy['total_number_of_words_in_q1'] + data_copy['total_number_of_words_in_q2']
        
        data_copy['number_of_unique_words_in_q1'] = data_copy['question1'].apply(lambda text : len(set(nltk.word_tokenize(text))))
        
        data_copy['number_of_unique_words_in_q2'] = data_copy['question2'].apply(lambda text : len(set(nltk.word_tokenize(text))))
        
        data_copy['sum_of_total_uinque_words_of_q1_and_q2'] = data_copy['number_of_unique_words_in_q1'] + data_copy['number_of_unique_words_in_q2']
        
        data_copy['ratio_of_total_unique_words_and_total_words'] = data_copy['sum_of_total_uinque_words_of_q1_and_q2']/data_copy['sum_of_total_words_of_q1_and_q2']
        
        data_copy['number_of_common_words_in_q1_and_q2'] = data_copy.apply(lambda x : len(set(x.question1.split()).intersection(set(x.question2.split()))) , axis=1)
        
        data_copy['ratio_of_common_words_of_q1q2_and_total_words'] = data_copy['number_of_common_words_in_q1_and_q2']/data_copy['sum_of_total_words_of_q1_and_q2']
        
        data_copy['ratio_of_common_words_of_q1q2_and_total_unique_words'] = data_copy['number_of_common_words_in_q1_and_q2']/data_copy['sum_of_total_uinque_words_of_q1_and_q2']
        
        data_copy['ratio_of_common_words_and_length_of_smaller_question'] = data_copy['number_of_common_words_in_q1_and_q2']/np.minimum(data_copy['length_of_q1'] , data_copy['length_of_q2'])
        
        data_copy['ratio_of_common_words_and_length_of_larger_question'] = data_copy['number_of_common_words_in_q1_and_q2']/np.maximum(data_copy['length_of_q1'] , data_copy['length_of_q2'])
        
        return data_copy
    
    def add_fuzzywuzzy_features(self , data):
        
        data_copy = data.copy()
        
        data_copy['fuzz_ratio'] = data_copy.apply(lambda x : fuzz.ratio(x.question1 , x.question2) , axis=1)
        
        data_copy['fuzz_partial_ratio'] = data_copy.apply(lambda x : fuzz.partial_ratio(x.question1 , x.question2) , axis=1)
    
        data_copy['fuzz_token_sort_ratio'] = data_copy.apply(lambda x : fuzz.token_sort_ratio(x.question1 , x.question2) , axis=1)
    
        data_copy['fuzz_token_set_ratio'] = data_copy.apply(lambda x : fuzz.token_set_ratio(x.question1 , x.question2) , axis=1)
        
        return data_copy

fe = FeatureEngineering(data__)
df = fe.feature_extraction(data__)
df = fe.add_fuzzywuzzy_features(df)

df.head()

df.shape

df.columns

"""## Exploratery Data Analysis"""

plt.figure(figsize=(15,5))

plt.subplot(1,2,1)

sns.boxplot(x='is_duplicate' , y='length_of_q1' , data=df)
plt.title('Box Plot - Length of Question 1')
plt.xlabel('Class label')
plt.ylabel('Number of characters')

plt.subplot(1,2,2)

sns.kdeplot(x='length_of_q1' , hue='is_duplicate' , data=df , shade=True)
plt.title('PDF - Length of qQuestion 1')
plt.xlabel('Number of characters')

plt.show()

plt.figure(figsize=(15,5))

plt.subplot(1,2,1)

sns.boxplot(x='is_duplicate' , y='length_of_q2' , data=df)
plt.title('Box Plot - Length of Question 2')
plt.xlabel('Class label')
plt.ylabel('Number of characters')

plt.subplot(1,2,2)

sns.kdeplot(x='length_of_q2' , hue='is_duplicate' , data=df , shade=True)
plt.title('PDF - Length of qQuestion 2')
plt.xlabel('Number of characters')

plt.show()

plt.figure(figsize=(15,5))

plt.subplot(1,2,1)

sns.boxplot(x='is_duplicate' , y='total_number_of_words_in_q1' , data=df)
plt.title('Box Plot - Total number of words in question1')
plt.xlabel('Class label')
plt.ylabel('Total number of words in question 1')

plt.subplot(1,2,2)

sns.kdeplot(x='total_number_of_words_in_q1' , hue='is_duplicate' , data=df , shade=False)
plt.title('PDF - Total number of words in question1')
plt.xlabel('Total number of words in question1')

plt.show()

plt.figure(figsize=(15,5))

plt.subplot(1,2,1)

sns.boxplot(x='is_duplicate' , y='total_number_of_words_in_q2' , data=df)
plt.title('Box Plot - Total number of words in question2')
plt.xlabel('Class label')
plt.ylabel('Total number of words in question 2')

plt.subplot(1,2,2)

sns.kdeplot(x='total_number_of_words_in_q2' , hue='is_duplicate' , data=df , shade=False)
plt.title('PDF - Total number of words in question2')
plt.xlabel('Total number of words in question2')

plt.show()

plt.figure(figsize=(15,5))

plt.subplot(1,2,1)

sns.boxplot(x='is_duplicate' , y='sum_of_total_words_of_q1_and_q2' , data=df)
plt.title('Box Plot - Total number of words in both question')
plt.xlabel('Class label')
plt.ylabel('Sum of words of question1 and question 2')

plt.subplot(1,2,2)

sns.kdeplot(x='sum_of_total_words_of_q1_and_q2' , hue='is_duplicate' , data=df , shade=False)
plt.title('PDF - Total number of words in both question')
plt.xlabel('Sum of words of question1 and question 2')

plt.show()

plt.figure(figsize=(15,5))

plt.subplot(1,2,1)

sns.boxplot(x='is_duplicate' , y='number_of_unique_words_in_q1' , data=df)
plt.title('Box Plot - Total number of unique words in question1')
plt.xlabel('Class label')
plt.ylabel('Total number of unique words in question 1')

plt.subplot(1,2,2)

sns.kdeplot(x='number_of_unique_words_in_q1' , hue='is_duplicate' , data=df , shade=False)
plt.title('PDF - Total number of unique words in question1')
plt.xlabel('Total number of unique words in question1')

plt.show()

plt.figure(figsize=(15,5))

plt.subplot(1,2,1)

sns.boxplot(x='is_duplicate' , y='number_of_unique_words_in_q2' , data=df)
plt.title('Box Plot - Total number of unique words in question2')
plt.xlabel('Class label')
plt.ylabel('Total number of unique words in question 2')

plt.subplot(1,2,2)

sns.kdeplot(x='number_of_unique_words_in_q2' , hue='is_duplicate' , data=df , shade=False)
plt.title('PDF - Total number of unique words in question2')
plt.xlabel('Total number of unique words in question2')

plt.show()

plt.figure(figsize=(15,5))

plt.subplot(1,2,1)

sns.boxplot(x='is_duplicate' , y='sum_of_total_uinque_words_of_q1_and_q2' , data=df)
plt.title('Box Plot - Total number of unique words in both question')
plt.xlabel('Class label')
plt.ylabel('Sum of unique words of question1 and question 2')

plt.subplot(1,2,2)

sns.kdeplot(x='sum_of_total_uinque_words_of_q1_and_q2' , hue='is_duplicate' , data=df , shade=False)
plt.title('PDF - Total number of unique words in both question')
plt.xlabel('Sum of unique words of question1 and question 2')

plt.show()

plt.figure(figsize=(15,5))

plt.subplot(1,2,1)

sns.boxplot(x='is_duplicate' , y='ratio_of_total_unique_words_and_total_words', data=df)
plt.title('Box Plot - Ratio of total unique words and total Words ')
plt.xlabel('Class label')
plt.ylabel('Ratio of total unique words and total words')

plt.subplot(1,2,2)

sns.kdeplot(x='ratio_of_total_unique_words_and_total_words', hue='is_duplicate' , data=df , shade=False)
plt.title('PDF - Ratio of total unique words and total Words ')
plt.xlabel('Ratio of total unique words and total words')

plt.show()

plt.figure(figsize=(15,5))

plt.subplot(1,2,1)

sns.boxplot(x='is_duplicate' , y='number_of_common_words_in_q1_and_q2', data=df)
plt.title('Box Plot - Commom words in both questions')
plt.xlabel('Class label')
plt.ylabel('number of common word')

plt.subplot(1,2,2)

sns.kdeplot(x='number_of_common_words_in_q1_and_q2', hue='is_duplicate' , data=df , shade=False)
plt.title('PDF - Commom words in both questions')
plt.xlabel('number of common word')

plt.show()

plt.figure(figsize=(15,5))

plt.subplot(1,2,1)

sns.boxplot(x='is_duplicate' , y='ratio_of_common_words_of_q1q2_and_total_words' , data=df)
plt.title('Box Plot - Ratio of common words and total words')
plt.xlabel('Class label')
plt.ylabel('Ratio of common words and total words')

plt.subplot(1,2,2)

sns.kdeplot(x='ratio_of_common_words_of_q1q2_and_total_words' , hue='is_duplicate' , data=df , shade=False)
plt.title('PDF - Ratio of common words and total words')
plt.xlabel('Ratio of common words and total words')

plt.show()

plt.figure(figsize=(15,5))

plt.subplot(1,2,1)

sns.boxplot(x='is_duplicate' , y='ratio_of_common_words_of_q1q2_and_total_unique_words' , data=df)
plt.title('Box Plot - Ratio of common words and total unique words')
plt.xlabel('Class label')
plt.ylabel('Ratio of common words and total unique words')

plt.subplot(1,2,2)

sns.kdeplot(x='ratio_of_common_words_of_q1q2_and_total_unique_words' , hue='is_duplicate' , data=df , shade=False)
plt.title('PDF - Ratio of common words and total unique words')
plt.xlabel('Ratio of common words and total unique words')

plt.show()

plt.figure(figsize=(15,5))

plt.subplot(1,2,1)

sns.boxplot(x='is_duplicate' , y='ratio_of_common_words_and_length_of_smaller_question' , data=df)
plt.title('Box Plot - Ratio of commons words and small length question')
plt.xlabel('Class label')
plt.ylabel('Ratio of commons words and small length question')

plt.subplot(1,2,2)

sns.kdeplot(x='ratio_of_common_words_and_length_of_smaller_question' , hue='is_duplicate' , data=df , shade=False)
plt.title('PDF - Ratio of commons words and small length question')
plt.xlabel('Ratio of commons words and small length question')

plt.show()

plt.figure(figsize=(15,5))

plt.subplot(1,2,1)

sns.boxplot(x='is_duplicate' , y='ratio_of_common_words_and_length_of_larger_question' , data=df)
plt.title('Box Plot - Ratio of commons words and large length question')
plt.xlabel('Class label')
plt.ylabel('Ratio of commons words and large length question')

plt.subplot(1,2,2)

sns.kdeplot(x='ratio_of_common_words_and_length_of_larger_question' , hue='is_duplicate' , data=df , shade=False)
plt.title('PDF - Ratio of commons words and large length question')
plt.xlabel('Ratio of commons words and large length question')

plt.show()

plt.figure(figsize=(15,5))

plt.subplot(1,2,1)

sns.boxplot(x='is_duplicate' , y='fuzz_ratio' , data=df)
plt.title('Box Plot - Fuzz Ratio')
plt.xlabel('Class label')
plt.ylabel('fazz ratio')

plt.subplot(1,2,2)

sns.kdeplot(x='fuzz_ratio' , hue='is_duplicate' , data=df , shade=False)
plt.title('PDF - Fuzz Ratio')
plt.xlabel('fuzz ratio')

plt.show()

plt.figure(figsize=(15,5))

plt.subplot(1,2,1)

sns.boxplot(x='is_duplicate' , y='fuzz_partial_ratio' , data=df)
plt.title('Box Plot - Fuzz Partial Ratio')
plt.xlabel('Class label')
plt.ylabel('fazz partial ratio')

plt.subplot(1,2,2)

sns.kdeplot(x='fuzz_partial_ratio' , hue='is_duplicate' , data=df , shade=False)
plt.title('PDF - Fuzz Partial Ratio')
plt.xlabel('fuzz partial ratio')

plt.show()

plt.figure(figsize=(15,5))

plt.subplot(1,2,1)

sns.boxplot(x='is_duplicate' , y='fuzz_token_sort_ratio' , data=df)
plt.title('Box Plot - Fuzz Token Sort Ratio')
plt.xlabel('Class label')
plt.ylabel('fazz token sort ratio')

plt.subplot(1,2,2)

sns.kdeplot(x='fuzz_token_sort_ratio' , hue='is_duplicate' , data=df , shade=False)
plt.title('PDF - Fuzz Token Sort Ratio')
plt.xlabel('fuzz token sort ratio')

plt.show()

plt.figure(figsize=(15,5))

plt.subplot(1,2,1)

sns.boxplot(x='is_duplicate' , y='fuzz_token_set_ratio' , data=df)
plt.title('Box Plot - Fuzz Token Set Ratio')
plt.xlabel('Class label')
plt.ylabel('fazz token set ratio')

plt.subplot(1,2,2)

sns.kdeplot(x='fuzz_token_set_ratio' , hue='is_duplicate' , data=df , shade=False)
plt.title('PDF - Fuzz Token Set Ratio')
plt.xlabel('fuzz token set ratio')

plt.show()

"""### Due to same distribution droping some features"""

#'ratio_of_common_words_of_q1q2_and_total_words' and 'ratio_of_common_words_of_q1q2_and_total_unique_words' 
# features are giving same distribution so droping 
# 'ratio_of_common_words_of_q1q2_and_total_unique_words' feature

df = df.drop('ratio_of_common_words_of_q1q2_and_total_unique_words' , axis=1)
df.shape

# 'fuzz_ratio' and 'fuzz_token_sort_ratio'
# features are giving same distribution so droping 'fuzz_ratio'

df = df.drop('fuzz_ratio' , axis=1)
df.shape

df['question1'] = df['question1'].apply(lambda text : str(text))
df['question2'] = df['question2'].apply(lambda text : str(text))

df.shape

"""### Showing Worlcloud"""

from wordcloud import WordCloud

duplicate = df[df['is_duplicate']==1]
non_duplicate = df[df['is_duplicate']==0]

duplicate = np.array([duplicate['question1'] , duplicate['question2']]).flatten()
non_duplicate = np.array([non_duplicate['question1'], non_duplicate['question2']]).flatten()

dup_str = ' '.join(duplicate)
non_dup_str = ' '.join(non_duplicate)

stop_words = set(stopwords.words('english'))

word_cloud = WordCloud(background_color='black' , max_words=len(dup_str) , stopwords=stop_words , width=600 , height=400)
word_cloud.generate(dup_str)

print('Word cloud for duplicate pairs')
plt.figure(figsize=(15,8))
plt.imshow(word_cloud, interpolation='bilinear')
plt.axis('off')

plt.show()

word_cloud = WordCloud(background_color='black' , max_words=len(dup_str) , stopwords=stop_words , width=600 , height=400)
word_cloud.generate(non_dup_str)

print('Word cloud for non duplicate pairs')
plt.figure(figsize=(15,8))
plt.imshow(word_cloud, interpolation='bilinear')
plt.axis('off')

plt.show()

"""## Vectorization"""

from numpy import vectorize
from sklearn.feature_extraction.text import TfidfVectorizer
from tqdm import tqdm
import spacy

nlp = spacy.load('en_core_web_sm')

# Merge texts
questions = list(data['question1'])+list(data['question2'])

tfidf = TfidfVectorizer(lowercase=False)
tfidf.fit_transform(questions)

#  dict key:word and value:tf-idf score
word2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))

vecs1 = []

# https://github.com/noamraph/tqdm
# tqdm is used to print the progress bar
for qu1 in tqdm(list(data['question1'])):
    doc1 = nlp(qu1) 
    # 384 is the number of dimensions of vectors 
    mean_vec1 = np.zeros([len(doc1), len(doc1[0].vector)])
    for word1 in doc1:
        # Word2Vec
        vec1 = word1.vector
        # Fetch df score
        try: idf = word2tfidf[str(word1)]
        except: idf = 0
        # Compute final vec
        mean_vec1 += vec1 * idf
    mean_vec1 = mean_vec1.mean(axis=0)
    vecs1.append(mean_vec1)
data['q1_feats_m'] = list(vecs1)

data.shape

vecs2 = []
for qu2 in tqdm(list(data['question2'])):
    doc2 = nlp(qu2) 
    mean_vec2 = np.zeros([len(doc1), len(doc2[0].vector)])
    for word2 in doc2:
        # Word2Vec
        vec2 = word2.vector
        # Fetch df score
        try: idf = word2tfidf[str(word2)]
        except: idf = 0
        # Compute final vec
        mean_vec2 += vec2 * idf
    mean_vec2 = mean_vec2.mean(axis=0)
    vecs2.append(mean_vec2)
data['q2_feats_m'] = list(vecs2)

data.shape

df1 = df.drop(['qid1' , 'qid2' , 'question1' , 'question2'] , axis=1)
df2 = data.drop(['qid1' , 'qid2' , 'question1' , 'question2' , 'is_duplicate'], axis=1)

print(df1.shape)
print(df2.shape)

df2_q1 = pd.DataFrame(df2.q1_feats_m.values.tolist() , index = df2.index)
df2_q2 = pd.DataFrame(df2.q2_feats_m.values.tolist() , index = df2.index)

print(df2_q1.shape)
print(df2_q2.shape)

df2_q1['id'] = df1['id']
df2_q2['id'] = df1['id']

df2 = df2_q1.merge(df2_q2 , on='id' , how='left')

df2.shape

result = df1.merge(df2 , on='id' , how='left')

result.shape

result.head()

result.isnull().sum().sum()

result = result.dropna()

result.shape

"""## Data splitting"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import RandomizedSearchCV

import xgboost as xgb
from xgboost import XGBClassifier
from sklearn.neighbors import KNeighborsClassifier

result = result.drop('id' , axis=1)

X = result.drop('is_duplicate' , axis=1)
Y = result['is_duplicate']

print(X.shape , Y.shape)

x_train , x_test , y_train , y_test = train_test_split(X , Y , test_size=0.3 , random_state=1)

print(x_train.shape , x_test.shape , y_train.shape , y_test.shape)

"""## Error Analysis"""

from sklearn.metrics import (
    accuracy_score,
    confusion_matrix,
    log_loss,
    precision_score,
    recall_score,
    f1_score,
)
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns

class EvaluateModel:
    def __init__(self, x_test, y_test, model):
        self.x_test = x_test
        self.y_test = y_test
        self.model = model

    def evaluate_model(self):
        print("Evaluating the model:- ")
        y_pred = self.model.predict(self.x_test)
        print("Accuracy Score:- ", accuracy_score(self.y_test, y_pred))
        print("Precision Score:- ", precision_score(self.y_test, y_pred))
        print("Recall Score:- ", recall_score(self.y_test, y_pred))
        print("F1 Score:- ", f1_score(self.y_test, y_pred))
        print(
            "Log Loss:- ", log_loss(self.y_test, self.model.predict_proba(self.x_test))
        )
        print("Completed evaluating the model")

    def plot_confusion_matrix(self, test_y, predict_y):
        confusion = confusion_matrix(test_y, predict_y)

        Recall = ((confusion.T) / (confusion.sum(axis=1))).T
        # divide each element of the confusion matrix with the sum of elements in that column

        Precision = confusion / confusion.sum(axis=0)
        # divide each element of the confusion matrix with the sum of elements in that row

        plt.figure(figsize=(20, 4))

        labels = [0, 1]
        cmap = sns.light_palette("blue")
        plt.subplot(1, 3, 1)
        sns.heatmap(
            confusion,
            annot=True,
            cmap=cmap,
            fmt=".3f",
            xticklabels=labels,
            yticklabels=labels,
        )
        plt.xlabel("Predicted Class")
        plt.ylabel("Original Class")
        plt.title("Confusion matrix")

        plt.subplot(1, 3, 2)
        sns.heatmap(
            Precision,
            annot=True,
            cmap=cmap,
            fmt=".3f",
            xticklabels=labels,
            yticklabels=labels,
        )
        plt.xlabel("Predicted Class")
        plt.ylabel("Original Class")
        plt.title("Precision matrix")

        plt.subplot(1, 3, 3)
        sns.heatmap(
            Recall,
            annot=True,
            cmap=cmap,
            fmt=".3f",
            xticklabels=labels,
            yticklabels=labels,
        )
        plt.xlabel("Predicted Class")
        plt.ylabel("Original Class")
        plt.title("Recall matrix")

        plt.show()

    def plot_roc_curve(self, test_y, predict_y):
        auroc = roc_auc_score(test_y, predict_y)
        print("AUROC Score:- ", auroc)
        fpr, tpr, _ = roc_curve(test_y, predict_y)
        plt.plot(
            fpr, tpr, linestyle="--", label="Prediction_for_lr (AUROC = %0.3f)" % auroc
        )
        plt.title("ROC Plot")
        plt.xlabel("False Positive Rate")
        plt.ylabel("True Positive Rate")
        plt.legend()
        plt.show()

"""### XG Boost Model"""

def Xgboost_model(x_train , y_train , fine_tuning=True):
    if fine_tuning:
        print("Started Finetuning the model:- ")
        n_estimators = [50, 100, 150, 200]
        max_depth = [2, 4, 6, 8]
        learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]
        subsample = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]
        colsample_bytree = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]
        min_child_weight = [1, 2, 3, 4, 5]
        gamma = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
        params = {
            "n_estimators": n_estimators,
            "max_depth": max_depth,
            "learning_rate": learning_rate,
            "subsample": subsample,
            "colsample_bytree": colsample_bytree,
            "min_child_weight": min_child_weight,
            "gamma": gamma,
        }
        xgb = XGBClassifier()
        clf = RandomizedSearchCV(xgb, params, cv=5, n_jobs=-1)
        clf.fit(x_train, y_train)
        print("Finished Hyperparameter search")
        return clf
    else:
        print("Started training the model:- ")
        xgb = XGBClassifier(
            learning_rate=0.3,
            max_delta_step=0,
            max_depth=8,
            min_child_weight=1,
            n_estimators=50,
            n_jobs=16,
            num_parallel_tree=1,
            random_state=0,
            reg_alpha=0,
            reg_lambda=1,
            scale_pos_weight=1,
            subsample=1.0,
            tree_method="exact",
            validate_parameters=1,
            verbosity=None,
        )
        xgb.fit(x_train, y_train)
        print("Completed training the model")
        return xgb

xgb = Xgboost_model(x_train , y_train , fine_tuning=False)

xgb_predict = xgb.predict(x_test)

evaluate_xgb = EvaluateModel(x_test , y_test , xgb)
evaluate_xgb.evaluate_model()

evaluate_xgb.plot_confusion_matrix(y_test , xgb_predict)

evaluate_xgb.plot_roc_curve(y_test , xgb_predict)

"""### SVM Model"""

# def svm_model(x_train , y_train , fine_tuning=True): 
#     if fine_tuning:  
#         print("Started Finetuning the model:- ")
#         C = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
#         gamma = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
#         params = {
#             "C": C,
#             "gamma": gamma,
#         }
#         svm = SVC()
#         clf = RandomizedSearchCV(svm, params, cv=5, n_jobs=-1)
#         clf.fit(x_train,y_train)
#         print("Finished Hyperparameter search")
#         return clf
#     else:
#         print("Started training the model:- ")
#         svm = SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
#             decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
#             max_iter=-1, probability=False, random_state=None, shrinking=True,
#             tol=0.001, verbose=False)
#         svm.fit(x_train, y_train)
#         print("Completed training the model")
#         return svm 
# svm = svm_model(x_train , y_train , fine_tuning=False)
# svm_predict = svm.predict(x_test)
# evaluate_svm = EvaluateModel(x_test , y_test , svm)
# evaluate_svm.evaluate_model()
# evaluate_svm.plot_confusion_matrix(y_test , svm_predict)
# evaluate_svm.plot_roc_curve(y_test , svm_predict)

"""### Logistic Regression"""

# def logistic_regression_model(x_train , y_train):
#     print("Training the model logistic regression model:- ")
#     log_reg = LogisticRegression(max_iter=1000)
#     log_reg.fit(x_train, y_train)

#     print("Completed training the model")
#     return log_reg
# lr = logistic_regression_model(x_train , y_train)
# lr_predict = lr.predict(x_test)
# evaluate_lr = EvaluateModel(x_test , y_test , lr)
# evaluate_lr.evaluate_model()
# evaluate_lr.plot_confusion_matrix(y_test , lr_predict)
# evaluate_lr.plot_roc_curve(y_test , lr_predict)